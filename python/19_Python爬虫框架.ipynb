{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python 爬虫框架\n",
        "\n",
        "本教程将学习Python中常用的爬虫库和框架，包括BeautifulSoup和Scrapy基础。\n",
        "\n",
        "## 1. BeautifulSoup - HTML/XML解析库\n",
        "\n",
        "BeautifulSoup是解析HTML和XML文档的库，适合快速开发简单的爬虫。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BeautifulSoup 基础（需要安装: pip install beautifulsoup4 lxml）\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "    \n",
        "    # 示例HTML\n",
        "    html_doc = \"\"\"\n",
        "    <html>\n",
        "    <head><title>测试页面</title></head>\n",
        "    <body>\n",
        "        <div class=\"content\">\n",
        "            <h1>标题</h1>\n",
        "            <p class=\"intro\">这是第一段</p>\n",
        "            <p>这是第二段</p>\n",
        "            <a href=\"https://example.com\">链接</a>\n",
        "        </div>\n",
        "        <ul>\n",
        "            <li>项目1</li>\n",
        "            <li>项目2</li>\n",
        "            <li>项目3</li>\n",
        "        </ul>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    \n",
        "    # 创建BeautifulSoup对象\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "    \n",
        "    # 获取标题\n",
        "    title = soup.title.string\n",
        "    print(f\"页面标题: {title}\")\n",
        "    \n",
        "    # 查找所有段落\n",
        "    paragraphs = soup.find_all('p')\n",
        "    print(f\"\\n找到 {len(paragraphs)} 个段落:\")\n",
        "    for p in paragraphs:\n",
        "        print(f\"  - {p.get_text()}\")\n",
        "    \n",
        "    # 查找特定class的元素\n",
        "    intro = soup.find('p', class_='intro')\n",
        "    if intro:\n",
        "        print(f\"\\nintro段落: {intro.get_text()}\")\n",
        "    \n",
        "    # 查找所有链接\n",
        "    links = soup.find_all('a')\n",
        "    print(f\"\\n找到 {len(links)} 个链接:\")\n",
        "    for link in links:\n",
        "        print(f\"  - {link.get_text()}: {link.get('href')}\")\n",
        "    \n",
        "    # 查找列表项\n",
        "    items = soup.find_all('li')\n",
        "    print(f\"\\n列表项:\")\n",
        "    for item in items:\n",
        "        print(f\"  - {item.get_text()}\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"BeautifulSoup未安装，使用 'pip install beautifulsoup4 lxml' 安装\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BeautifulSoup 选择器（类似CSS选择器）\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    \n",
        "    html = \"\"\"\n",
        "    <div class=\"container\">\n",
        "        <div class=\"item\" id=\"item1\">\n",
        "            <h2>商品1</h2>\n",
        "            <p class=\"price\">¥99.00</p>\n",
        "        </div>\n",
        "        <div class=\"item\" id=\"item2\">\n",
        "            <h2>商品2</h2>\n",
        "            <p class=\"price\">¥199.00</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    \n",
        "    # 使用CSS选择器\n",
        "    print(\"使用CSS选择器:\")\n",
        "    \n",
        "    # 选择所有class为item的div\n",
        "    items = soup.select('div.item')\n",
        "    print(f\"找到 {len(items)} 个商品:\")\n",
        "    for item in items:\n",
        "        name = item.select_one('h2').get_text()\n",
        "        price = item.select_one('.price').get_text()\n",
        "        print(f\"  - {name}: {price}\")\n",
        "    \n",
        "    # 选择特定ID的元素\n",
        "    item1 = soup.select_one('#item1')\n",
        "    if item1:\n",
        "        print(f\"\\nID为item1的商品: {item1.select_one('h2').get_text()}\")\n",
        "    \n",
        "    # 选择所有价格\n",
        "    prices = soup.select('.price')\n",
        "    print(f\"\\n所有价格:\")\n",
        "    for price in prices:\n",
        "        print(f\"  - {price.get_text()}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"BeautifulSoup未安装\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 爬虫最佳实践\n",
        "\n",
        "编写爬虫时需要注意的要点。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.robotparser import RobotFileParser\n",
        "\n",
        "print(\"爬虫最佳实践:\")\n",
        "print(\"\\n1. 遵守robots.txt协议:\")\n",
        "print(\"   - 检查网站的robots.txt文件\")\n",
        "print(\"   - 遵守爬取规则\")\n",
        "print(\"   - 尊重网站的使用条款\")\n",
        "\n",
        "print(\"\\n2. 设置合理的请求间隔:\")\n",
        "print(\"   - 不要过于频繁地请求\")\n",
        "print(\"   - 使用 time.sleep() 添加延迟\")\n",
        "print(\"   - 避免对服务器造成压力\")\n",
        "\n",
        "print(\"\\n3. 设置User-Agent:\")\n",
        "print(\"   - 模拟浏览器访问\")\n",
        "print(\"   - 避免被识别为爬虫\")\n",
        "\n",
        "print(\"\\n4. 错误处理:\")\n",
        "print(\"   - 处理网络错误\")\n",
        "print(\"   - 处理解析错误\")\n",
        "print(\"   - 使用try-except捕获异常\")\n",
        "\n",
        "print(\"\\n5. 数据存储:\")\n",
        "print(\"   - 及时保存爬取的数据\")\n",
        "print(\"   - 避免重复爬取\")\n",
        "print(\"   - 使用数据库存储\")\n",
        "\n",
        "# 示例：遵守robots.txt的爬虫\n",
        "def check_robots_txt(url):\n",
        "    \"\"\"检查robots.txt\"\"\"\n",
        "    rp = RobotFileParser()\n",
        "    rp.set_url(f\"{url}/robots.txt\")\n",
        "    rp.read()\n",
        "    return rp.can_fetch('*', url)\n",
        "\n",
        "print(\"\\n6. 使用Session:\")\n",
        "print(\"   - 保持会话状态\")\n",
        "print(\"   - 处理cookies\")\n",
        "print(\"   - 提高效率\")\n",
        "\n",
        "# 示例：使用Session\n",
        "def crawl_with_session(url):\n",
        "    \"\"\"使用Session爬取\"\"\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    })\n",
        "    \n",
        "    try:\n",
        "        response = session.get(url, timeout=10)\n",
        "        response.raise_for_status()  # 检查HTTP错误\n",
        "        time.sleep(1)  # 添加延迟\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"请求失败: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"\\n示例函数已定义，展示了基本的爬虫实践\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
