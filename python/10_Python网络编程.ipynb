{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python 网络编程\n",
        "\n",
        "本教程将学习Python中的网络编程，包括socket编程、HTTP请求、Web爬虫等内容。\n",
        "\n",
        "## 1. Socket 编程基础 - TCP 服务器和客户端\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TCP服务器代码已定义，可以在后台运行\n"
          ]
        }
      ],
      "source": [
        "import socket\n",
        "\n",
        "# TCP 服务器示例\n",
        "def tcp_server():\n",
        "    \"\"\"创建一个简单的TCP服务器\"\"\"\n",
        "    # 创建socket对象\n",
        "    # AF_INET 表示使用 IPv4 地址\n",
        "    # SOCK_STREAM 表示使用 TCP 协议\n",
        "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    \n",
        "    # 设置地址重用\n",
        "    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "    \n",
        "    # 绑定地址和端口\n",
        "    host = 'localhost'\n",
        "    port = 8888\n",
        "    server_socket.bind((host, port))\n",
        "    \n",
        "    # 开始监听\n",
        "    # 5 表示最大连接数\n",
        "    server_socket.listen(5)\n",
        "    print(f\"服务器启动，监听 {host}:{port}\")\n",
        "    \n",
        "    while True:\n",
        "        # 接受客户端连接\n",
        "        client_socket, address = server_socket.accept()\n",
        "        print(f\"收到来自 {address} 的连接\")\n",
        "        \n",
        "        # 接收数据\n",
        "        data = client_socket.recv(1024).decode('utf-8')\n",
        "        print(f\"接收到数据: {data}\")\n",
        "        \n",
        "        # 发送响应\n",
        "        response = f\"服务器收到: {data}\"\n",
        "        client_socket.send(response.encode('utf-8'))\n",
        "        \n",
        "        # 关闭客户端连接\n",
        "        client_socket.close()\n",
        "        \n",
        "        # 演示用，只处理一次连接\n",
        "        break\n",
        "    \n",
        "    server_socket.close()\n",
        "\n",
        "# 注意：在实际使用中，服务器应该在单独的线程或进程中运行\n",
        "# print(\"TCP服务器代码（需要单独运行）:\")\n",
        "print(\"TCP服务器代码已定义，可以在后台运行\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TCP客户端代码已定义\n"
          ]
        }
      ],
      "source": [
        "# TCP 客户端示例\n",
        "def tcp_client():\n",
        "    \"\"\"创建一个简单的TCP客户端\"\"\"\n",
        "    # 创建socket对象\n",
        "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    \n",
        "    # 连接到服务器\n",
        "    host = 'localhost'\n",
        "    port = 8888\n",
        "    \n",
        "    try:\n",
        "        client_socket.connect((host, port))\n",
        "        print(f\"已连接到服务器 {host}:{port}\")\n",
        "        \n",
        "        # 发送数据\n",
        "        message = \"Hello, Server!\"\n",
        "        client_socket.send(message.encode('utf-8'))\n",
        "        print(f\"发送: {message}\")\n",
        "        \n",
        "        # 接收响应\n",
        "        response = client_socket.recv(1024).decode('utf-8')\n",
        "        print(f\"服务器响应: {response}\")\n",
        "        \n",
        "    except ConnectionRefusedError:\n",
        "        print(\"无法连接到服务器，请确保服务器正在运行\")\n",
        "    finally:\n",
        "        client_socket.close()\n",
        "\n",
        "print(\"TCP客户端代码已定义\")\n",
        "# 注意：运行客户端前需要先启动服务器\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HTTP 请求 - urllib 和 requests 库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用 urllib（Python标准库）\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.error import URLError\n",
        "import json\n",
        "\n",
        "# 简单的GET请求\n",
        "try:\n",
        "    # 发送GET请求\n",
        "    response = urlopen('https://httpbin.org/get')\n",
        "    data = response.read().decode('utf-8')\n",
        "    print(\"使用 urllib 发送GET请求:\")\n",
        "    print(f\"状态码: {response.status}\")\n",
        "    print(f\"响应内容（前200字符）: {data[:200]}\")\n",
        "except URLError as e:\n",
        "    print(f\"请求失败: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用 requests 发送GET请求:\n",
            "状态码: 200\n",
            "响应JSON: {'args': {'key': 'value'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-692403fa-46a9467b0d4289074bb95fbd'}, 'origin': '209.50.227.209', 'url': 'https://httpbin.org/get?key=value'}\n",
            "\n",
            "POST请求状态码: 200\n",
            "POST响应: {'args': {}, 'data': '', 'files': {}, 'form': {'age': '25', 'name': '张三'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Content-Length': '30', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-692403fc-589aaae206b80b820c6ba741'}, 'json': None, 'origin': '209.50.227.209', 'url': 'https://httpbin.org/post'}\n",
            "\n",
            "带自定义请求头: {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Host': 'httpbin.org', 'User-Agent': 'MyApp/1.0', 'X-Amzn-Trace-Id': 'Root=1-692403fd-6fb79a0904689a996e0af6e6'}\n"
          ]
        }
      ],
      "source": [
        "# 使用 requests 库（需要安装: pip install requests）\n",
        "# 这是一个更强大和易用的HTTP库\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "    \n",
        "    # GET请求\n",
        "    response = requests.get('https://httpbin.org/get', params={'key': 'value'})\n",
        "    print(\"使用 requests 发送GET请求:\")\n",
        "    print(f\"状态码: {response.status_code}\")\n",
        "    print(f\"响应JSON: {response.json()}\")\n",
        "    \n",
        "    # POST请求\n",
        "    post_data = {'name': '张三', 'age': 25}\n",
        "    response = requests.post('https://httpbin.org/post', data=post_data)\n",
        "    print(f\"\\nPOST请求状态码: {response.status_code}\")\n",
        "    print(f\"POST响应: {response.json()}\")\n",
        "    \n",
        "    # 设置请求头\n",
        "    headers = {'User-Agent': 'MyApp/1.0'}\n",
        "    response = requests.get('https://httpbin.org/headers', headers=headers)\n",
        "    print(f\"\\n带自定义请求头: {response.json()['headers']}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"requests库未安装，使用 'pip install requests' 安装\")\n",
        "except Exception as e:\n",
        "    print(f\"请求失败: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 简单的Web爬虫示例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 简单的网页爬虫（演示用）\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import URLError\n",
        "\n",
        "def simple_crawler(url):\n",
        "    \"\"\"简单的网页爬虫示例\"\"\"\n",
        "    try:\n",
        "        # 发送请求\n",
        "        response = urlopen(url)\n",
        "        html = response.read().decode('utf-8')\n",
        "        \n",
        "        # 提取标题\n",
        "        title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)\n",
        "        if title_match:\n",
        "            print(f\"网页标题: {title_match.group(1)}\")\n",
        "        \n",
        "        # 提取所有链接（简单示例）\n",
        "        links = re.findall(r'href=[\"\\'](.*?)[\"\\']', html)\n",
        "        print(f\"\\n找到 {len(links)} 个链接\")\n",
        "        print(\"前5个链接:\")\n",
        "        for link in links[:5]:\n",
        "            print(f\"  - {link}\")\n",
        "            \n",
        "    except URLError as e:\n",
        "        print(f\"爬取失败: {e}\")\n",
        "\n",
        "# 演示（使用一个公开的测试网站）\n",
        "# simple_crawler('https://httpbin.org/html')\n",
        "print(\"网页爬虫函数已定义，可以使用 simple_crawler(url) 进行爬取\")\n",
        "print(\"注意：实际爬虫需要处理更多情况，如反爬虫、JavaScript渲染等\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 使用 urllib.parse 处理URL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL解析:\n",
            "协议: https\n",
            "域名: www.example.com\n",
            "路径: /path/to/page\n",
            "查询参数: param1=value1&param2=value2\n",
            "片段: \n",
            "\n",
            "URL拼接: https://www.example.com/page.html\n",
            "\n",
            "查询字符串: name=%E5%BC%A0%E4%B8%89&age=25&city=%E5%8C%97%E4%BA%AC\n",
            "\n",
            "URL编码: 你好世界 -> %E4%BD%A0%E5%A5%BD%E4%B8%96%E7%95%8C\n",
            "URL解码: %E4%BD%A0%E5%A5%BD%E4%B8%96%E7%95%8C -> 你好世界\n"
          ]
        }
      ],
      "source": [
        "from urllib.parse import urlparse, urljoin, urlencode, quote, unquote\n",
        "\n",
        "# 解析URL\n",
        "url = \"https://www.example.com/path/to/page?param1=value1&param2=value2\"\n",
        "parsed = urlparse(url)\n",
        "print(\"URL解析:\")\n",
        "print(f\"协议: {parsed.scheme}\")\n",
        "print(f\"域名: {parsed.netloc}\")\n",
        "print(f\"路径: {parsed.path}\")\n",
        "print(f\"查询参数: {parsed.query}\")\n",
        "print(f\"片段: {parsed.fragment}\")\n",
        "\n",
        "# URL拼接\n",
        "base_url = \"https://www.example.com\"\n",
        "relative_path = \"/page.html\"\n",
        "full_url = urljoin(base_url, relative_path)\n",
        "print(f\"\\nURL拼接: {full_url}\")\n",
        "\n",
        "# 构建查询字符串\n",
        "params = {'name': '张三', 'age': 25, 'city': '北京'}\n",
        "query_string = urlencode(params)\n",
        "print(f\"\\n查询字符串: {query_string}\")\n",
        "\n",
        "# URL编码和解码\n",
        "original = \"你好世界\"\n",
        "encoded = quote(original)\n",
        "decoded = unquote(encoded)\n",
        "print(f\"\\nURL编码: {original} -> {encoded}\")\n",
        "print(f\"URL解码: {encoded} -> {decoded}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 使用上下文管理器 (Context Managers)\n",
        "\n",
        "使用 `with` 语句可以自动管理 socket 的关闭，避免资源泄漏。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "端口 80 是开放的\n",
            "端口 81 是开放的\n"
          ]
        }
      ],
      "source": [
        "import socket\n",
        "\n",
        "def check_port(host, port):\n",
        "    \"\"\"检查端口是否开放\"\"\"\n",
        "    try:\n",
        "        # 使用 with 语句自动关闭 socket\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.settimeout(1)\n",
        "            # connect_ex 返回 0 表示端口是开放的，否则是关闭的\n",
        "            result = s.connect_ex((host, port))\n",
        "            if result == 0:\n",
        "                print(f\"端口 {port} 是开放的\")\n",
        "            else:\n",
        "                print(f\"端口 {port} 是关闭的\")\n",
        "    except Exception as e:\n",
        "        print(f\"检查出错: {e}\")\n",
        "\n",
        "check_port('www.baidu.com', 80)\n",
        "check_port('www.baidu.com', 81)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 异步网络编程 (Asyncio)\n",
        "\n",
        "对于高并发的网络应用，`asyncio` 是更好的选择。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始获取: http://example.com/1\n",
            "开始获取: http://example.com/2\n",
            "开始获取: http://example.com/3\n",
            "完成获取: http://example.com/1\n",
            "完成获取: http://example.com/2\n",
            "完成获取: http://example.com/3\n",
            "所有结果: ['Content of http://example.com/1', 'Content of http://example.com/2', 'Content of http://example.com/3']\n",
            "在 Jupyter Notebook 中，可以直接使用 await main() 运行\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "async def fetch_url(url):\n",
        "    \"\"\"模拟异步获取URL\"\"\"\n",
        "    print(f\"开始获取: {url}\")\n",
        "    await asyncio.sleep(1)  # 模拟网络延迟\n",
        "    print(f\"完成获取: {url}\")\n",
        "    return f\"Content of {url}\"\n",
        "\n",
        "async def main():\n",
        "    urls = ['http://example.com/1', 'http://example.com/2', 'http://example.com/3']\n",
        "    # 并发执行任务\n",
        "    tasks = [fetch_url(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    print(f\"所有结果: {results}\")\n",
        "\n",
        "# 在 Jupyter 中运行 asyncio\n",
        "await main()\n",
        "print(\"在 Jupyter Notebook 中，可以直接使用 await main() 运行\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 健壮的错误处理\n",
        "\n",
        "网络请求经常失败，需要实现重试机制。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def robust_request(url, max_retries=3):\n",
        "    \"\"\"带有重试机制的请求模拟\"\"\"\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            print(f\"尝试请求 {url} (第 {i+1} 次)...\")\n",
        "            # 模拟随机失败\n",
        "            if random.random() < 0.7:\n",
        "                raise ConnectionError(\"连接超时\")\n",
        "            \n",
        "            print(\"请求成功！\")\n",
        "            return \"Success Data\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"请求失败: {e}\")\n",
        "            if i < max_retries - 1:\n",
        "                wait_time = 2 ** i  # 指数退避\n",
        "                print(f\"等待 {wait_time} 秒后重试...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"达到最大重试次数，放弃\")\n",
        "                raise\n",
        "\n",
        "# robust_request('http://unstable-api.com')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
