{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python 网络编程\n",
        "\n",
        "本教程将学习Python中的网络编程，包括socket编程、HTTP请求、Web爬虫等内容。\n",
        "\n",
        "## 1. Socket 编程基础 - TCP 服务器和客户端\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import socket\n",
        "\n",
        "# TCP 服务器示例\n",
        "def tcp_server():\n",
        "    \"\"\"创建一个简单的TCP服务器\"\"\"\n",
        "    # 创建socket对象\n",
        "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    \n",
        "    # 设置地址重用\n",
        "    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "    \n",
        "    # 绑定地址和端口\n",
        "    host = 'localhost'\n",
        "    port = 8888\n",
        "    server_socket.bind((host, port))\n",
        "    \n",
        "    # 开始监听\n",
        "    server_socket.listen(5)\n",
        "    print(f\"服务器启动，监听 {host}:{port}\")\n",
        "    \n",
        "    while True:\n",
        "        # 接受客户端连接\n",
        "        client_socket, address = server_socket.accept()\n",
        "        print(f\"收到来自 {address} 的连接\")\n",
        "        \n",
        "        # 接收数据\n",
        "        data = client_socket.recv(1024).decode('utf-8')\n",
        "        print(f\"接收到数据: {data}\")\n",
        "        \n",
        "        # 发送响应\n",
        "        response = f\"服务器收到: {data}\"\n",
        "        client_socket.send(response.encode('utf-8'))\n",
        "        \n",
        "        # 关闭客户端连接\n",
        "        client_socket.close()\n",
        "        \n",
        "        # 演示用，只处理一次连接\n",
        "        break\n",
        "    \n",
        "    server_socket.close()\n",
        "\n",
        "# 注意：在实际使用中，服务器应该在单独的线程或进程中运行\n",
        "# print(\"TCP服务器代码（需要单独运行）:\")\n",
        "print(\"TCP服务器代码已定义，可以在后台运行\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TCP 客户端示例\n",
        "def tcp_client():\n",
        "    \"\"\"创建一个简单的TCP客户端\"\"\"\n",
        "    # 创建socket对象\n",
        "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    \n",
        "    # 连接到服务器\n",
        "    host = 'localhost'\n",
        "    port = 8888\n",
        "    \n",
        "    try:\n",
        "        client_socket.connect((host, port))\n",
        "        print(f\"已连接到服务器 {host}:{port}\")\n",
        "        \n",
        "        # 发送数据\n",
        "        message = \"Hello, Server!\"\n",
        "        client_socket.send(message.encode('utf-8'))\n",
        "        print(f\"发送: {message}\")\n",
        "        \n",
        "        # 接收响应\n",
        "        response = client_socket.recv(1024).decode('utf-8')\n",
        "        print(f\"服务器响应: {response}\")\n",
        "        \n",
        "    except ConnectionRefusedError:\n",
        "        print(\"无法连接到服务器，请确保服务器正在运行\")\n",
        "    finally:\n",
        "        client_socket.close()\n",
        "\n",
        "print(\"TCP客户端代码已定义\")\n",
        "# 注意：运行客户端前需要先启动服务器\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HTTP 请求 - urllib 和 requests 库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用 urllib（Python标准库）\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.error import URLError\n",
        "import json\n",
        "\n",
        "# 简单的GET请求\n",
        "try:\n",
        "    # 发送GET请求\n",
        "    response = urlopen('https://httpbin.org/get')\n",
        "    data = response.read().decode('utf-8')\n",
        "    print(\"使用 urllib 发送GET请求:\")\n",
        "    print(f\"状态码: {response.status}\")\n",
        "    print(f\"响应内容（前200字符）: {data[:200]}\")\n",
        "except URLError as e:\n",
        "    print(f\"请求失败: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用 requests 库（需要安装: pip install requests）\n",
        "# 这是一个更强大和易用的HTTP库\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "    \n",
        "    # GET请求\n",
        "    response = requests.get('https://httpbin.org/get', params={'key': 'value'})\n",
        "    print(\"使用 requests 发送GET请求:\")\n",
        "    print(f\"状态码: {response.status_code}\")\n",
        "    print(f\"响应JSON: {response.json()}\")\n",
        "    \n",
        "    # POST请求\n",
        "    post_data = {'name': '张三', 'age': 25}\n",
        "    response = requests.post('https://httpbin.org/post', data=post_data)\n",
        "    print(f\"\\nPOST请求状态码: {response.status_code}\")\n",
        "    print(f\"POST响应: {response.json()}\")\n",
        "    \n",
        "    # 设置请求头\n",
        "    headers = {'User-Agent': 'MyApp/1.0'}\n",
        "    response = requests.get('https://httpbin.org/headers', headers=headers)\n",
        "    print(f\"\\n带自定义请求头: {response.json()['headers']}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"requests库未安装，使用 'pip install requests' 安装\")\n",
        "except Exception as e:\n",
        "    print(f\"请求失败: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 简单的Web爬虫示例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 简单的网页爬虫（演示用）\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import URLError\n",
        "\n",
        "def simple_crawler(url):\n",
        "    \"\"\"简单的网页爬虫示例\"\"\"\n",
        "    try:\n",
        "        # 发送请求\n",
        "        response = urlopen(url)\n",
        "        html = response.read().decode('utf-8')\n",
        "        \n",
        "        # 提取标题\n",
        "        title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)\n",
        "        if title_match:\n",
        "            print(f\"网页标题: {title_match.group(1)}\")\n",
        "        \n",
        "        # 提取所有链接（简单示例）\n",
        "        links = re.findall(r'href=[\"\\'](.*?)[\"\\']', html)\n",
        "        print(f\"\\n找到 {len(links)} 个链接\")\n",
        "        print(\"前5个链接:\")\n",
        "        for link in links[:5]:\n",
        "            print(f\"  - {link}\")\n",
        "            \n",
        "    except URLError as e:\n",
        "        print(f\"爬取失败: {e}\")\n",
        "\n",
        "# 演示（使用一个公开的测试网站）\n",
        "# simple_crawler('https://httpbin.org/html')\n",
        "print(\"网页爬虫函数已定义，可以使用 simple_crawler(url) 进行爬取\")\n",
        "print(\"注意：实际爬虫需要处理更多情况，如反爬虫、JavaScript渲染等\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 使用 urllib.parse 处理URL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse, urljoin, urlencode, quote, unquote\n",
        "\n",
        "# 解析URL\n",
        "url = \"https://www.example.com/path/to/page?param1=value1&param2=value2\"\n",
        "parsed = urlparse(url)\n",
        "print(\"URL解析:\")\n",
        "print(f\"协议: {parsed.scheme}\")\n",
        "print(f\"域名: {parsed.netloc}\")\n",
        "print(f\"路径: {parsed.path}\")\n",
        "print(f\"查询参数: {parsed.query}\")\n",
        "print(f\"片段: {parsed.fragment}\")\n",
        "\n",
        "# URL拼接\n",
        "base_url = \"https://www.example.com\"\n",
        "relative_path = \"/page.html\"\n",
        "full_url = urljoin(base_url, relative_path)\n",
        "print(f\"\\nURL拼接: {full_url}\")\n",
        "\n",
        "# 构建查询字符串\n",
        "params = {'name': '张三', 'age': 25, 'city': '北京'}\n",
        "query_string = urlencode(params)\n",
        "print(f\"\\n查询字符串: {query_string}\")\n",
        "\n",
        "# URL编码和解码\n",
        "original = \"你好世界\"\n",
        "encoded = quote(original)\n",
        "decoded = unquote(encoded)\n",
        "print(f\"\\nURL编码: {original} -> {encoded}\")\n",
        "print(f\"URL解码: {encoded} -> {decoded}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
